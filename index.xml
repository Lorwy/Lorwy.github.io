<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lorwy&#39;s blog</title>
    <link>http://lorwy.github.io/</link>
    <description>Recent content on Lorwy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>lorwy</copyright>
    <lastBuildDate>Mon, 22 Oct 2018 21:38:52 +0800</lastBuildDate>
    
        <atom:link href="http://lorwy.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://lorwy.github.io/about/</link>
      <pubDate>Mon, 22 Oct 2018 21:38:52 +0800</pubDate>
      
      <guid>http://lorwy.github.io/about/</guid>
      
        <description>&lt;p&gt;这是一个学习、分享的地方！&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>运用梯度下降优化逻辑回归模型</title>
      <link>http://lorwy.github.io/post/note/logistic_regression_with_gradient_descent/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://lorwy.github.io/post/note/logistic_regression_with_gradient_descent/</guid>
      
        <description>

&lt;h1 id=&#34;逻辑回归简介&#34;&gt;逻辑回归简介&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;逻辑回归并不属于回归算法，而是经典的二分类算法，也可以处理多分类问题。&lt;/p&gt;

&lt;p&gt;机器学习算法选择：先逻辑回归再用复杂的，能简单就用简单的&lt;/p&gt;

&lt;p&gt;拿到一个分类问题，首选逻辑回归试试，看看效果如何，再用复杂的分类算法试试，看看是否有逻辑回归效果更好&lt;/p&gt;

&lt;p&gt;注：&lt;strong&gt;逻辑回归的决策边界：可以使非线性的&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;具体算法&#34;&gt;具体算法&lt;/h1&gt;

&lt;p&gt;目标：建立分类器来求出 $\theta$&lt;/p&gt;

&lt;p&gt;算法步骤：
- 实现 &lt;code&gt;sigmoid(z)&lt;/code&gt; 函数
- 实现 &lt;code&gt;h(x)&lt;/code&gt; 函数
- 从数据中提取出特征 &lt;code&gt;X&lt;/code&gt; 和标签 &lt;code&gt;y&lt;/code&gt;
- 实现 &lt;code&gt;cost function&lt;/code&gt;
- 实现偏导数 &lt;code&gt;gradient()&lt;/code&gt;
- 进行参数更新 &lt;code&gt;descent&lt;/code&gt;
- 最后就求出了 $\theta$&lt;/p&gt;

&lt;h2 id=&#34;sigmoid函数&#34;&gt;sigmoid函数&lt;/h2&gt;

&lt;p&gt;$$g(z) = \frac{1}{1+e^{-z}}$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lorwy.github.io/image/sigmoid.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;函数定义:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(z):
    return 1 / (1 + np.exp(-z))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;h-x&#34;&gt;h(x)&lt;/h2&gt;

&lt;p&gt;$$h(x) = \theta^T X$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hx(X, theta):
    return sigmoid(np.dot(X, theta.T))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如：
$$
\begin{array}{ccc}
\begin{pmatrix}\theta&lt;em&gt;{0} &amp;amp; \theta&lt;/em&gt;{1} &amp;amp; \theta&lt;em&gt;{2}\end{pmatrix} &amp;amp; \times &amp;amp; \begin{pmatrix}1&lt;br /&gt;
x&lt;/em&gt;{1}&lt;br /&gt;
x&lt;em&gt;{2}
\end{pmatrix}\end{array}=\theta&lt;/em&gt;{0}+\theta&lt;em&gt;{1}x&lt;/em&gt;{1}+\theta&lt;em&gt;{2}x&lt;/em&gt;{2}
$$&lt;/p&gt;

&lt;h2 id=&#34;从数据中提取出特征-x-和标签-y&#34;&gt;从数据中提取出特征 &lt;code&gt;X&lt;/code&gt; 和标签 &lt;code&gt;y&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;提取出 &lt;code&gt;X&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 并根据 &lt;code&gt;X&lt;/code&gt; 的shape来初始化 &lt;code&gt;theta&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;实现损失函数-cost-function&#34;&gt;实现损失函数 &lt;code&gt;cost function&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;将对数似然函数去负号&lt;/p&gt;

&lt;p&gt;$$
D(h&lt;em&gt;\theta(x), y) = -y\log(h&lt;/em&gt;\theta(x)) - (1-y)\log(1-h&lt;em&gt;\theta(x))
$$
求平均损失
$$
J(\theta)=\frac{1}{m}\sum&lt;/em&gt;{i=1}^{m} D(h_\theta(x_i), y_i)
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cost(X, y, theta):
    left = np.multiply(-y, np.log(hx(X, theta)))
    right = np.multiply(1, -y, np.log(1 - hx(X, theta)))
    return np.sum(left - right) / (len(X))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实现偏导数-gradient-计算梯度&#34;&gt;实现偏导数 &lt;code&gt;gradient()&lt;/code&gt;计算梯度&lt;/h2&gt;

&lt;p&gt;$$
\frac{\partial J}{\partial \theta&lt;em&gt;j}=-\frac{1}{m}\sum&lt;/em&gt;{i=1}^n (y&lt;em&gt;i - h&lt;/em&gt;\theta (x&lt;em&gt;i))x&lt;/em&gt;{ij}
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient(X, y, theta):
    grad = np.zeros(theta.shape)
    error = (hx(X, theta) - y).ravel()
    #print(theta.ravel())
    for j in range(len(theta.ravel())):
        term = np.multiply(error, X[:,j])
        grad[0,j] = np.sum(term) / len(X)
        
    return grad
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;实现descent方法-并进行gradient-descent&#34;&gt;实现descent方法，并进行Gradient descent&lt;/h2&gt;

&lt;p&gt;3种不同的梯度下降方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;批量梯度下降（Gradient descent）&lt;/li&gt;
&lt;li&gt;随机梯度下降（stochastic Gradient descent）&lt;/li&gt;
&lt;li&gt;小批量梯度下降（mini-bath Gradient descent）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3种不同的停止策略&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;循环次数达到阈值时&lt;/li&gt;
&lt;li&gt;代价函数低于阈值时&lt;/li&gt;
&lt;li&gt;梯度下降小于阈值时（$\varDelta \theta_j$）&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;STOP_ITER = 0
STOP_COST = 1
STOP_GRAD = 2

# type：停止策略 value：当前值 threshold：阈值
def stopCriterion(type, value, threshold):
    if type == STOP_ITER:
        return value &amp;gt; threshold
    elif type == STOP_COST:
        return abs(value[-1] - value[-2]) &amp;lt; threshold
    elif type == STOP_GRAD:
        return np.linalg.norm(value) &amp;lt; threshold
    else:
        return YES
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;打乱数据&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy.random
def shuffleData(data):
    np.random.shuffle(data)
    cols = data.shape[1]
    X = data[:,0:cols-1]
    y = data[:,cols-1:]
    return X, y
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;gradient-descent函数&#34;&gt;Gradient descent函数&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time
# data：带标签的training data
# theta: 跟特征一样维度的初始化的theta行向量
# batchSize：每次更新theta时用的数据个数，用来选择对应的3种梯度下降算法
#           bathchSize = X.shape[0] ：就是批量梯度下降
#           batchSize = 1 ：就是随机梯度下降
#           1 &amp;lt; batchSize &amp;lt; X.shape[0] ： 就是小批量梯度下降
def descent(data, theta, batchSize, stopType, thresh, alpha):
    
    init_time = time.time() # 用来计算梯度下降耗时
    i = 0 # 迭代次数
    k = 0 # batch
    X, y = shuffleData(data) 
    grad = np.zeros(theta.shape) # 计算的梯度（就是对各个theta[j]值的偏导数）
    costs = [cost(X, y, theta)] # 计算代价函数(先把训练前的第一个存进来)
    
    while True:
        grad = gradient(X[k:k+batchSize], y[k:k+batchSize], theta)
        k += batchSize 
        if k&amp;gt;= n:
            # 当跑满了循环次数后
            k = 0
            X, y = shuffleData(data) # 打乱数据
        theta = theta - alpha*grad # 参数更新
        costs.append(cost(X, y, theta)) # 计算新的代价函数
        i += 1
        
        if stopType == STOP_ITER:
            value = i
        elif stopType == STOP_COST:
            value = costs
        elif stopType == STOP_GRAD:
            value = grad
        if stopCriterion(stopType, value, thresh):
            break
    return theta, i-1, costs, grad, time.time() - init_time
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;测试方法&#34;&gt;测试方法&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def runExpe(data, theta, batchSize, stopType, thresh, alpha):
    theta, iter, costs, grad, dur = descent(data, theta, batchSize, stopType, thresh, alpha)
    #
    name = &amp;quot;Original&amp;quot; if(data[:,1]&amp;gt;2).sum() &amp;gt; 1 else &amp;quot;Scaled&amp;quot;
    #
    name += &amp;quot; data - learning rate:{} - &amp;quot;.format(alpha)
    #
    if batchSize == n:
        strDescType = &amp;quot;Gradient&amp;quot;
    elif batchSize == 1:
        strDescType = &amp;quot;Stochastic&amp;quot;
    else:
        strDescType = &amp;quot;Mini-batch({})&amp;quot;.format(batchSize)
    name += strDescType + &amp;quot; descent - Stop: &amp;quot;
    #
    if stopType == STOP_ITER:
        strStop = &amp;quot;{} iterations&amp;quot;.format(thresh)
    elif stopType == STOP_COST:
        strStop = &amp;quot;costs change &amp;lt; {}&amp;quot;.format(thresh)
    else:
        strStop = &amp;quot;gradient norm &amp;lt; {}&amp;quot;.format(thresh)
        
    name += strStop;
    print(&amp;quot;***{}\nTheta: {} - Iter: {} - Last Cost: {:03.2f} - Duration: {:03.2f}s&amp;quot;.format(
    name, theta, iter, costs[-1], dur))
    
    fig, ax = plt.subplots(figsize=(12,4))
    ax.plot(np.arange(len(costs)), costs, &#39;r&#39;)
    ax.set_xlabel(&#39;Iterations&#39;)
    ax.set_ylabel(&#39;Cost&#39;)
    ax.set_title(name.upper() + &#39;- Error VS. Iteration&#39;)
    return theta
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = X.shape[0]
runExpe(orig_data, theta, n, STOP_ITER, thresh=5000, alpha=0.000001)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://lorwy.github.io/image/logistic_regression_test.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>iOS面试之道_架构篇_Note</title>
      <link>http://lorwy.github.io/post/note/ios%E9%9D%A2%E8%AF%95%E4%B9%8B%E9%81%93_%E6%9E%B6%E6%9E%84%E7%AF%87_note/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://lorwy.github.io/post/note/ios%E9%9D%A2%E8%AF%95%E4%B9%8B%E9%81%93_%E6%9E%B6%E6%9E%84%E7%AF%87_note/</guid>
      
        <description>

&lt;p&gt;每个类、结构体、方法、变量的存在都应该遵循单一职责原则。&lt;/p&gt;

&lt;h1 id=&#34;mvc架构的优缺点&#34;&gt;MVC架构的优缺点&lt;/h1&gt;

&lt;p&gt;优点有2：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;代码总量少&lt;/li&gt;
&lt;li&gt;简单易懂&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：（造成的原因主要因为&lt;strong&gt;视图层和控制器层高度耦合&lt;/strong&gt;）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;代码过于集中（ViewController功能太过沉重）

&lt;ul&gt;
&lt;li&gt;交互&lt;/li&gt;
&lt;li&gt;视图更新&lt;/li&gt;
&lt;li&gt;布局&lt;/li&gt;
&lt;li&gt;Model数据获取及修改&lt;/li&gt;
&lt;li&gt;导航路由&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;难以进行测试&lt;/li&gt;
&lt;li&gt;难以扩展（*ViewController太过笨重*）&lt;/li&gt;
&lt;li&gt;Model层过于简单&lt;/li&gt;
&lt;li&gt;网络层无从安放&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;: 过于笼统的代码分配，导致除了数据或者视图外的类、结构、方法等都将被放到ViewController中，造成ViewController过于臃肿，与view和无法解耦合&lt;/p&gt;

&lt;h1 id=&#34;mvcs&#34;&gt;MVCS&lt;/h1&gt;

&lt;p&gt;对MVC的一种优化，S即Store，就是数据处理的，可以是数据持久化相关的代码、数据筛选分类之类的等等，无处安放的网络请求代码也可以放到这里&lt;/p&gt;

&lt;h1 id=&#34;viper架构简介&#34;&gt;VIPER架构简介&lt;/h1&gt;

&lt;p&gt;VIPER架构由5部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;View&lt;/li&gt;
&lt;li&gt;Interactor&lt;/li&gt;
&lt;li&gt;Presenter&lt;/li&gt;
&lt;li&gt;Entity&lt;/li&gt;
&lt;li&gt;Router&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示意图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lorwy.github.io/image/VIPER.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;各模块说明&#34;&gt;各模块说明&lt;/h4&gt;

&lt;h5 id=&#34;视图层-view&#34;&gt;视图层（View）&lt;/h5&gt;

&lt;p&gt;与MVP、MVVM一样，它包含与UI相关的一切操作，接受用户交互信息但不处理，而是传递给展示层&lt;/p&gt;

&lt;h5 id=&#34;展示层-presenter&#34;&gt;展示层（Presenter）&lt;/h5&gt;

&lt;p&gt;与MVP的Presenter或者MVVM的ViewModel功能类似。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;响应view传来的交互操作请求&lt;/li&gt;
&lt;li&gt;不对数据源修改，有修改需求的话就向Interactor发送请求&lt;/li&gt;
&lt;li&gt;链接路由层&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;路由层-router&#34;&gt;路由层（Router）&lt;/h5&gt;

&lt;p&gt;负责界面跳转、组件切换&lt;/p&gt;

&lt;h5 id=&#34;数据管理层-interactor&#34;&gt;数据管理层（Interactor）&lt;/h5&gt;

&lt;p&gt;负责处理数据源信息&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;网络请求&lt;/li&gt;
&lt;li&gt;数据传输&lt;/li&gt;
&lt;li&gt;缓存、存储&lt;/li&gt;
&lt;li&gt;生成实例等&lt;/li&gt;
&lt;li&gt;一些从中间层和模型层的一些逻辑差不多被剥离至此&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;模型层-entity&#34;&gt;模型层（Entity）&lt;/h5&gt;

&lt;p&gt;很简单，只有下面两个东西&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化方法&lt;/li&gt;
&lt;li&gt;属性相关的get/set方法&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>观 sunnyxx iOS 面试视频总结</title>
      <link>http://lorwy.github.io/post/note/sunnyxx_ios_note/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://lorwy.github.io/post/note/sunnyxx_ios_note/</guid>
      
        <description>

&lt;h1 id=&#34;review-简历&#34;&gt;Review 简历&lt;/h1&gt;

&lt;h4 id=&#34;关于项目总结&#34;&gt;关于项目总结&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;主要对这几块进行总结，要结合技术Tips总结出来&lt;/strong&gt;
- 项目描述
- 项目规模
- 其中职责
- 涉及的技术栈
- 遇到的挑战及如何解决的&lt;/p&gt;

&lt;h4 id=&#34;专业技能&#34;&gt;专业技能：&lt;/h4&gt;

&lt;p&gt;例举技术名词时写出用其做了什么，如：
熟练应用objc runtime技术，曾使用它做了ViewController进入和退出时的&lt;strong&gt;AOP埋点&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;更多信息来体现特别&#34;&gt;更多信息来体现特别&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Blog&lt;/li&gt;
&lt;li&gt;GitHub&lt;/li&gt;
&lt;li&gt;技术探索&lt;/li&gt;
&lt;li&gt;对技术的个人理解&lt;/li&gt;
&lt;li&gt;做过的有意思的事儿（与技术相关的）&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;理解基本的应用架构&#34;&gt;理解基本的应用架构&lt;/h4&gt;

&lt;p&gt;MVC和MVVM理解是否正确&lt;/p&gt;

&lt;h2 id=&#34;几个面试题举例&#34;&gt;几个面试题举例&lt;/h2&gt;

&lt;h3 id=&#34;sr-内容管理&#34;&gt;[SR]内容管理&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Stack 和 Heap 分别的使用，如何管理？&lt;/li&gt;
&lt;li&gt;ARC 是如何实现的？&lt;/li&gt;
&lt;li&gt;Autorelease 对象何时释放？&lt;/li&gt;
&lt;li&gt;AutoreleasePoll是如何实现的？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;sr-理解-class-与对象模型&#34;&gt;[SR] 理解 Class 与对象模型&lt;/h3&gt;

&lt;h3 id=&#34;sr-理解-runloop&#34;&gt;[SR] 理解 Runloop&lt;/h3&gt;

&lt;h3 id=&#34;关于block的调用&#34;&gt;关于block的调用&lt;/h3&gt;

&lt;h3 id=&#34;sr-深入理解消息机制&#34;&gt;[SR] 深入理解消息机制&lt;/h3&gt;

&lt;p&gt;从写下[obj foo] 这行代码直到运行时 foo 被调用，中间都发生了什么？
&lt;strong&gt;掌握&lt;/strong&gt;：objc_msgSend 的关键调用，后续如何通过 selector 从 isa 找到 IMP，若运行时没找到 foo 会如何？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;精通&lt;/strong&gt;：编译器如何编译成 objc_msgSend、消息 cache 机制、消息转发机制、objc_msgSend 的各个版本、objc_msgSend的实现、跳板机制等&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>